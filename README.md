# assistive-mouse-capstone
Empowering those who cannot use computer mouses/stylus' to be able to use them with only a webcam and some basic gestures.


## MediaPipe
### Installation
Follow the instructions in `mediapipe/README.md`  
Note that this is using MediaPipe v0.7.0 with Bazel v1.2.1

### Building Source
```
cd mediapipe
./scripts/build.sh
```

### Running Demo 1
```
cd mediapipe
./scripts/run_demo1.sh
```

### Running Demo 2
```
cd mediapipe
./scripts/run_demo2.sh
```

### Networking
Protocol: UDP  
IP Address: `127.0.0.1`  
Ports: `2000`, `3000`, `4000`

The 21 key points of the hand can be received over ports `2000` or `3000`.
The data is transmitted as a string formatted as `"x_i, y_i, z_i;"` where `1 <= i <= 21`.
Note that each entry in the tuple is a floating-point number between 0 and 1.

Hand tracking information can be received over port `4000`.
The data is transmitted as a string formatted as `"x_cent, y_cent, x_rect, y_rect, w_rect, h_rect;"`.
`(x_cent, y_cent)` is the location of the centroid of the hand expressed as floating-point numbers.
`(x_rect, y_rect, w_rect, h_rect)` is the location and size of the bounding box expressed as floating-point numbers.
Note that this information is only available when running Demo 2.

## Gesture Learning
TODO

## Mouse Control
TODO

## Testing
TODO

## Important Files/Directories
TODO  
`mediapipe/mediapipe/calculators/util/landmark_forwarder_calculator.cc`: forwards 21 key points over UDP  
`mediapipe/mediapipe/calculators/util/hand_tracking_calculator.cc`: hand segmentation and tracking with OpenCV  
`gesture_learning/keypoints.py`: utility functions to manipulate key points generated by MediaPipe  
`gesture_learning/GestureRecognition.py`: runs a gesture learning model in real-time alongside MediaPipe  
`gesture_learning/template.py`: template for training a gesture learning model  
`gesture_learning/data/`: contains gesture datasets  
`gesture_learning/models/`: contains gesture learning models  
